{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea49dc0d7e497e19",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea7fe2861aab74",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Definition\n",
    "\n",
    "Given a linear model that predicts $f_{w,b}(x^{(i)})$:\n",
    "$$f_{w,b}(x^{(i)}) = wx^{(i)} + b \\tag{1}$$\n",
    "In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. The measure is called the $cost$, $J(w,b)$. In training you measure the cost over all of our training samples $x^{(i)},y^{(i)}$\n",
    "$$J(w,b) = \\frac{1}{2m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2\\tag{2}$$ \n",
    "\n",
    "In lecture, *gradient descent* was described as:\n",
    "\n",
    "$$\\begin{align*} \\text{repeat}&\\text{ until convergence:} \\; \\lbrace \\newline\n",
    "\\;  w &= w -  \\alpha \\frac{\\partial J(w,b)}{\\partial w} \\tag{3}  \\; \\newline \n",
    " b &= b -  \\alpha \\frac{\\partial J(w,b)}{\\partial b}  \\newline \\rbrace\n",
    "\\end{align*}$$\n",
    "where, parameters $w$, $b$ are updated simultaneously.  \n",
    "The gradient is defined as:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial J(w,b)}{\\partial w}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})x^{(i)} \\tag{4}\\\\\n",
    "  \\frac{\\partial J(w,b)}{\\partial b}  &= \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)}) \\tag{5}\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Here *simultaniously* means that you calculate the partial derivatives for all the parameters before updating any of the parameters.\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c1bb747d95179e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Gradient Function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b53e1e6f6440e293",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:29:40.173650Z",
     "start_time": "2024-05-23T11:29:40.056047Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import app.regression as ar\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.axhline(0, color='b')\n",
    "plt.axvline(0, color='b')\n",
    "\n",
    "train_data = [ar.Point(-9, -10.3), ar.Point(-7.5, -5), ar.Point(-5, -4.75), ar.Point(3, 2), \n",
    "              ar.Point(4, 5), ar.Point(5, 7), ar.Point(6, 8.3), ar.Point(7.5, 8.1)]\n",
    "\n",
    "for train_datum in train_data:\n",
    "    plt.plot(train_datum.x, train_datum.y, \"s\", color = 'b')\n",
    "\n",
    "ax.set_title('Sample ')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "\n",
    "def gradient(train_data, feature: ar.Feature):\n",
    "    # Number of training examples\n",
    "    gradient_a = 0\n",
    "    gradient_b = 0\n",
    "\n",
    "    for pt in train_data:\n",
    "        act_y = ar.actual_y(pt.x, feature)\n",
    "        gradient_a_i = (act_y - pt.y) * pt.x\n",
    "        gradient_b_i = (act_y - pt.y)\n",
    "        gradient_a += gradient_a_i\n",
    "        gradient_b += gradient_b_i\n",
    "        \n",
    "\n",
    "    gradient_a = gradient_a / len(train_data)\n",
    "    gradient_b = gradient_b / len(train_data)\n",
    "\n",
    "    return gradient_a, gradient_b\n",
    "\n",
    "gradients = gradient(train_data, ar.Feature(1, 1))\n",
    "print(f\"Gradients are {gradients}\")\n",
    "\n",
    "xs = [x for x in range(-10, 10)]\n",
    "ys = [gradients[0] * x + gradients[1]  for x in xs]\n",
    "\n",
    "plt.plot(xs, ys, color='g')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117973969c606db9",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## Calculation \n",
    "\n",
    "The following section depicts gradient descent calculation using different approaches\n",
    "\n",
    "### Calculation Using Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dda5752a1c1928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T11:49:01.124525Z",
     "start_time": "2024-05-24T11:47:07.950344Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from app.regression import Point, Feature, actual_y, cost\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "plt.axhline(0, color='b')\n",
    "plt.axvline(0, color='b')\n",
    "\n",
    "train_data = [Point(-9, -10.3), Point(-7.5, -5), Point(-5, -4.75), Point(3, 2), \n",
    "              Point(4, 5), Point(5, 7), Point(6, 8.3), Point(7.5, 8.1)]\n",
    "\n",
    "for train_datum in train_data:\n",
    "    plt.plot(train_datum.x, train_datum.y, \"s\", color='b')\n",
    "\n",
    "ax.set_title('Sample ')\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "\n",
    "\n",
    "def gradient(train_data, feature: Feature):\n",
    "    # Number of training examples\n",
    "    gradient_a = 0\n",
    "    gradient_b = 0\n",
    "\n",
    "    for pt in train_data:\n",
    "        act_y = actual_y(pt.x, feature)\n",
    "        gradient_a_i = (act_y - pt.y) * pt.x\n",
    "        gradient_b_i = (act_y - pt.y)\n",
    "        gradient_a += gradient_a_i\n",
    "        gradient_b += gradient_b_i\n",
    "        \n",
    "\n",
    "    gradient_a = gradient_a / len(train_data)\n",
    "    gradient_b = gradient_b / len(train_data)\n",
    "\n",
    "    return gradient_a, gradient_b\n",
    "\n",
    "gradients = gradient(train_data, Feature(1, 1))\n",
    "print(f\"Gradients are {gradients}\")\n",
    "\n",
    "num_iters = 10000\n",
    "alpha = 0.01\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "for i in range(num_iters):\n",
    "    # Calculate the gradient and update the parameters using gradient_function\n",
    "    ll = Feature(a, b)\n",
    "    cst = cost(train_data, ll)\n",
    "\n",
    "    (gradient_a, gradient_b)  = gradient(train_data, ll)\n",
    "\n",
    "    b = b - alpha * gradient_b                            \n",
    "    a = a - alpha * gradient_a                            \n",
    "\n",
    "    if i% math.ceil(num_iters/10) == 0:\n",
    "        print(f\"Iteration {i:4}: Cost {cst} \",\n",
    "              f\"Gradient a: {gradient_a}, Gradient B: {gradient_b}  \",\n",
    "              f\"line: {ll}\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b94949092b895b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "## Calculation Using Numpy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4adbdb038f6f4f69",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T11:50:26.337084Z",
     "start_time": "2024-05-24T11:50:26.206204Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "xs = np.array([-9, -7.5, -5, 3, 4, 5, 6, 7.5])\n",
    "ys = np.array([-10.3, -5, -4.75, 2, 5, 7, 8.3, 8.1])\n",
    "\n",
    "def cost(x, y, a_in, b_in):\n",
    "    m = len(xs)\n",
    "    guess = x * a_in + b_in\n",
    "    errys_sq = (guess - y) ** 2\n",
    "    err_sum = errys_sq.sum() / (2 * m)\n",
    "    return err_sum\n",
    "    \n",
    "def gradient(x, y, a_in, b_in):\n",
    "    # Number of training examples\n",
    "    \n",
    "    guess_y = x * a_in + b_in\n",
    "    ga = ((guess_y - y) * x).sum()\n",
    "    gb = (guess_y - y).sum()\n",
    "\n",
    "    m = len(xs)\n",
    "\n",
    "    ga = ga / m\n",
    "    gb = gb / m\n",
    "\n",
    "    return ga, gb\n",
    "\n",
    "gradients = gradient(xs, ys, 1, 1)\n",
    "print(f\"Gradients are {gradients}\")\n",
    "\n",
    "num_iters = 10000\n",
    "alpha = 0.01\n",
    "a = 0\n",
    "b = 0\n",
    "\n",
    "for i in range(num_iters):\n",
    "    # Calculate the gradient and update the parameters using gradient_function\n",
    "    cst = cost(xs, ys, a, b)\n",
    "    (gradient_a, gradient_b)  = gradient(xs, ys, a, b)\n",
    "\n",
    "    b = b - (alpha * gradient_b)                            \n",
    "    a = a - (alpha * gradient_a)                            \n",
    "\n",
    "    if i% math.ceil(num_iters/10) == 0:\n",
    "        print(f\"Iteration {i:4}: Cost {cst} \",\n",
    "              f\"Gradient a: {gradient_a}, Gradient B: {gradient_b}  \",\n",
    "              f\"line: {a}-{b}\")\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0cf840f89914f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Calculation Using Implemented Algorithm\n",
    "The app section implemented the algorithm on gradient_descent.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49c156646ee3f57",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:31:22.790505Z",
     "start_time": "2024-05-23T11:31:22.776829Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from app.gradient_descent import Point, Feature\n",
    "from app.gradient_descent import GradientDescent\n",
    "\n",
    "train_data = [Point(-9, -10.3), Point(-7.5, -5), Point(-5, -4.75), Point(3, 2),\n",
    "              Point(4, 5), Point(5, 7), Point(6, 8.3), Point(7.5, 8.1)]\n",
    "\n",
    "gd = GradientDescent(train_data, epochs=800)  # epochs=10000, learning_rate=0.01\n",
    "init_feature = Feature(0, 0)\n",
    "result = gd.optimize(init_feature) # Feature(0, -1, 0)\n",
    "optimized_feature = result[-1]['optimized_feature']\n",
    "print(f\"Optimized feature {init_feature} to {optimized_feature}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79026a6b444c788d",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "### Calculation Using sciKit-Learn\n",
    "\n",
    "SciKit-learn package is used to run the calculation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2d0c90b170ba534",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T11:31:26.583828Z",
     "start_time": "2024-05-23T11:31:26.578154Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "reg = linear_model.LinearRegression()\n",
    "\n",
    "xs = [[-9], [-7.5], [-5], [3], [4], [5], [6], [7.5]]\n",
    "ys = [[-10.3], [-5], [-4.75], [2], [5], [7], [8.3], [8.1]]\n",
    "\n",
    "reg.fit(xs, ys)\n",
    "\n",
    "print(f\"Result coef : {reg.coef_} and intercept is {reg.intercept_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf8404247c1e68e",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
